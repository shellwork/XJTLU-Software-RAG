import{_ as t,c as o,a0 as a,o as r}from"./chunks/framework.DgZgdyI2.js";const m=JSON.parse('{"title":"Question -Answer System","description":"","frontmatter":{},"headers":[],"relativePath":"docs/functions/chat.md","filePath":"docs/functions/chat.md"}'),s={name:"docs/functions/chat.md"};function n(d,e,i,c,l,u){return r(),o("div",null,e[0]||(e[0]=[a('<h1 id="question-answer-system" tabindex="-1">Question -Answer System <a class="header-anchor" href="#question-answer-system" aria-label="Permalink to &quot;Question -Answer System&quot;">​</a></h1><p>The chat file is based on the LangChain framework, combining vector databases with large language models to build a question-answering system. It can provide retrieval-based answers (RAG, Retrieval-Augmented Generation) or model-generated responses based on the user&#39;s input.</p><h2 id="key-functionalities" tabindex="-1">Key Functionalities: <a class="header-anchor" href="#key-functionalities" aria-label="Permalink to &quot;Key Functionalities:&quot;">​</a></h2><h3 id="_1-imported-libraries" tabindex="-1">1. Imported Libraries: <a class="header-anchor" href="#_1-imported-libraries" aria-label="Permalink to &quot;1. Imported Libraries:&quot;">​</a></h3><p>· <strong><code>Chroma</code></strong>: Used for managing a vector database, supporting persistent storage and similarity-based searches.</p><p>· <strong><code>ChatPromptTemplate</code></strong>: Used to create and format dialogue prompt templates.</p><p>· <strong><code>get_model</code></strong> and <strong><code>get_embedding_function</code></strong>: These functions, from the custom <code>model_selector</code> module, are responsible for fetching the model and embedding function.</p><p>· <strong>Path</strong> and <strong>sys</strong>: Handle paths to ensure correct import of configurations and modules.</p><p>· <strong><code>config</code></strong>: From the <code>config</code> module, imports <code>RAG_PROMPT_TEMPLATE</code> and <code>CHROMA_PATH</code>, which refer to the dialogue template and the path for vector database storage.</p><br><h3 id="_2-generate-response-function" tabindex="-1">2. <code>generate_response</code> Function: <a class="header-anchor" href="#_2-generate-response-function" aria-label="Permalink to &quot;2. `generate_response` Function:&quot;">​</a></h3><p>This is the core function of the system, generating a response based on user input. It combines vector database search with a language model&#39;s generation capabilities.</p><h4 id="parameters" tabindex="-1">Parameters: <a class="header-anchor" href="#parameters" aria-label="Permalink to &quot;Parameters:&quot;">​</a></h4><p>· <code>prompt</code>: The user&#39;s input question or dialogue.</p><p>· <code>tools</code>: Optional parameter, used to specify if certain tools are employed (not implemented in the given code).</p><p>· <code>model</code>: The name of the model to use, defaulting to &quot;default&quot;.</p><p>· <code>use_self_model</code>: Specifies whether to generate responses using a custom model. If False, only retrieval-based document chunks are returned. If <code>True</code>, the function combines the language model&#39;s generated dialogue.</p><p>· <code>use_local_model</code>: Specifies whether to use a local model for inference.</p><br><h3 id="_3-vector-database-query" tabindex="-1">3. Vector Database Query: <a class="header-anchor" href="#_3-vector-database-query" aria-label="Permalink to &quot;3. Vector Database Query:&quot;">​</a></h3><p>· The function first calls <code>get_embedding_function()</code> to retrieve the embedding function and uses it along with the <code>CHROMA_PATH</code> to initialize a Chroma vector database.</p><p>· Through <code>db.similarity_search_with_relevance_scores(prompt, k=3)</code>, the database performs a similarity search and returns the document chunks most relevant to the user&#39;s input, along with relevance scores.</p><br><h3 id="_4-two-response-modes" tabindex="-1">4. Two Response Modes: <a class="header-anchor" href="#_4-two-response-modes" aria-label="Permalink to &quot;4. Two Response Modes:&quot;">​</a></h3><br><h5 id="retrieval-mode-use-self-model-false" tabindex="-1">Retrieval Mode (<code>use_self_model=False</code>): <a class="header-anchor" href="#retrieval-mode-use-self-model-false" aria-label="Permalink to &quot;Retrieval Mode (`use_self_model=False`):&quot;">​</a></h5><p>If custom model generation is not used, the function returns only the most relevant document chunks and their reference information.</p><p>· If the highest similarity score is greater than or equal to 0.7, relevant document chunks and their sources are returned.</p><p>· Otherwise, empty document chunks and references are returned.</p><h5 id="dialogue-mode-use-self-model-true" tabindex="-1">Dialogue Mode (<code>use_self_model=True</code>): <a class="header-anchor" href="#dialogue-mode-use-self-model-true" aria-label="Permalink to &quot;Dialogue Mode (`use_self_model=True`):&quot;">​</a></h5><p>If a custom model is used to generate dialogue, it further combines the retrieved document chunks with the model&#39;s generated answer.</p><p>· First, it calls <code>get_model</code> to fetch the specified model and generates a response from the model.</p><p>· If relevant retrieval results exist (score &gt;= 0.7), the document chunks are formatted as context and used along with the model&#39;s response to create a final answer using a template.</p><p>· If no highly relevant results are found, it returns the model-generated answer directly.</p><br><h3 id="_5-return-structure" tabindex="-1">5. Return Structure: <a class="header-anchor" href="#_5-return-structure" aria-label="Permalink to &quot;5. Return Structure:&quot;">​</a></h3><p>The function returns a dictionary containing the generated answer (<code>answer</code>) or the retrieved document chunks (<code>chunks</code>), as well as their reference sources (<code>references</code>).</p>',37)]))}const p=t(s,[["render",n]]);export{m as __pageData,p as default};
